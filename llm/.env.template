# ==============================================================================
# vLLM DYNAMIC CONFIGURATION TEMPLATE
# Documentation: https://docs.vllm.ai/en/latest/models/engine_args.html
# ==============================================================================

# --- 1. ESSENTIAL CONFIGURATION (REQUIRED) ------------------------------------
# The only strictly required fields to boot the container.

# Hugging Face Token (Required for gated models like Llama 3, StarCoder, etc.)
# Get one here: https://huggingface.co/settings/tokens
HF_TOKEN=hf_write_your_token_here

# The exact repo ID from Hugging Face
MODEL_NAME=nvidia/Qwen3-Next-80B-A3B-Thinking-NVFP4

# Service Port (Default: 8000 or 12001)
PORT=12001

# --- 2. CUSTOM SCRIPT OVERRIDE (OPTIONAL) -------------------------------------
# If set, this bypasses the standard vLLM startup and runs your script instead.
# Useful for debugging or running completely different backends.
# CUSTOM_SCRIPT=/scripts/my_debug_script.sh

# --- 3. HARDWARE & MEMORY OPTIMIZATION ----------------------------------------
# Tuning these is critical for performance on Blackwell/H100 vs older cards.

# Percentage of GPU RAM to reserve for vLLM (0.0 - 1.0).
# Lower this if you get OOM errors. 
# Default: 0.9 (90%)
GPU_MEMORY_UTILIZATION=0.80

# Data type for the KV Cache.
# 'auto'    = vLLM decides (usually fp16)
# 'fp8'     = Recommended for H100/Blackwell (Reduces RAM usage by ~50%)
# 'fp8_e5m2' / 'fp8_e4m3' = specific FP8 formats
KV_CACHE_DTYPE=fp8

# Maximum Context Length.
# If the model supports 128k but you only have 24GB VRAM, lower this to avoid OOM.
# Examples: 4096, 8192, 32768, 131072
MAX_MODEL_LEN=131072

# Quantization method if not automatically detected.
# Options: awq, gptq, sane, marlin, bitsandbytes
# QUANTIZATION=

# --- 4. DISTRIBUTED INFERENCE -------------------------------------------------
# Required for models larger than a single GPU's VRAM.

# Number of GPUs to split the model across (Tensor Parallelism).
# Must be a divisor of the total GPUs available (1, 2, 4, 8).
TENSOR_PARALLEL_SIZE=1

# Number of nodes/machines to split across (Pipeline Parallelism).
# PIPELINE_PARALLEL_SIZE=1

# Distributed execution backend.
# 'ray' (Recommended for multi-node), 'mp' (multiprocessing, faster for single-node)
# DISTRIBUTED_EXECUTOR_BACKEND=mp

# --- 5. LOGIC, TOOL CALLING & PARSING -----------------------------------------
# Configuration for specific model capabilities.

# Parser for Tool/Function calling.
# Options: 'hermes' (Qwen/Hermes), 'mistral', 'llama3_json'
# TOOL_CALL_PARSER=hermes

# Parser for Reasoning/Chain-of-Thought models (DeepSeek R1, Qwen-Coder).
# Options: 'deepseek_r1', 'qwen3_coder'
# REASONING_PARSER=qwen3_coder

# Chat template format.
# 'auto' is usually best. Use 'chatml' or 'mistral' if auto fails.
# CHAT_TEMPLATE_CONTENT_FORMAT=auto

# --- 6. ADVANCED SCHEDULING & PERFORMANCE -------------------------------------
# Fine-grained tuning for high-throughput production environments.

# Maximum number of concurrent requests processed in a batch.
# Higher = better throughput, Lower = better latency.
# MAX_NUM_SEQS=256

# Split prefill (prompt processing) into smaller chunks to prevent stalling.
# ENABLE_CHUNKED_PREFILL=true

# Maximum tokens processed per iteration.
# MAX_NUM_BATCHED_TOKENS=8192

# Enable caching of common prefixes (e.g., system prompts).
# Greatly improves performance for agents/chatbots with long system prompts.
# ENABLE_PREFIX_CACHING=true

# --- 7. LoRA (LOW-RANK ADAPTATION) CONFIG -------------------------------------
# Enable this if you want to use multiple adapters on top of a base model.

# Enable LoRA support
# ENABLE_LORA=false

# Max LoRA rank (Must be >= the rank of your adapters).
# MAX_LORA_RANK=16

# Max concurrent adapters
# MAX_LORAS=1

# --- 8. SPECULATIVE DECODING (EXPERIMENTAL) -----------------------------------
# Accelerate generation by using a smaller draft model.

# Path to the smaller draft model
# SPECULATIVE_MODEL=

# Number of tokens the draft model guesses per step
# NUM_SPECULATIVE_TOKENS=5

# --- 9. THE ESCAPE HATCH ------------------------------------------------------
# Any argument not covered above can be added here.
# Example: "--seed 42 --enforce-eager --block-size 16"
VLLM_EXTRA_ARGS=